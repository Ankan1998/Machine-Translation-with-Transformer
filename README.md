# <p align = "center">*Machine Translation With Transformer*</p>

<br>

*TRANSFORMER* is introduced by Vaswani et al in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762). It is like the *Alexnet moment of Computer Vision* for NLP. From then, it became S-O-T-A architecture, with passing of time different improved version of these architecture is developed like *BERT*, *RobERTa*, *GPT* etc.
